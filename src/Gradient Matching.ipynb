{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50768 # Size of vocabulary\n",
    "DATA_PER_CLASS = 10\n",
    "SYNTHETIC_SIZE = 120\n",
    "DEVICE=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_syn = torch.randint(\n",
    "    high=VOCAB_SIZE,\n",
    "    size=(120, 128), \n",
    "    requires_grad=True, \n",
    "    device=DEVICE,\n",
    "    dtype=torch.float,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[35756., 48711., 15608.,  ..., 14561., 26030., 27185.],\n",
       "        [12290.,  4941., 33795.,  ..., 42057.,  7235., 24361.],\n",
       "        [31802., 42214., 48225.,  ..., 19891., 30722., 49903.],\n",
       "        ...,\n",
       "        [ 7481., 16925., 26879.,  ..., 16968., 33761., 10144.],\n",
       "        [10534., 30321., 11366.,  ..., 23064., 43659., 13331.],\n",
       "        [19262., 25335., 36413.,  ..., 42492., 29445., 37697.]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def decode_embeddings(embeddings: torch.nn.Embedding, embedded_data: torch.Tensor):\n",
    "    num_sentences = embedded_data.shape[0]\n",
    "    sentence_len = embedded_data.shape[1]\n",
    "\n",
    "    sentences = torch.zeros(num_sentences, sentence_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        sentence = torch.cdist(embedded_data[i, :, :].to(DEVICE), embeddings.weight.to(DEVICE), p=2)\n",
    "        sentences[i] = sentence.argmin(-1)\n",
    "    \n",
    "    return sentences.type(torch.long)\n",
    "\n",
    "\n",
    "def distance_wb(gwr, gws):\n",
    "    shape = gwr.shape\n",
    "    if len(shape) == 4: # conv, out*in*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "    elif len(shape) == 3:  # layernorm, C*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
    "    elif len(shape) == 2: # linear, out*in\n",
    "        tmp = 'do nothing'\n",
    "    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n",
    "        gwr = gwr.reshape(1, shape[0])\n",
    "        gws = gws.reshape(1, shape[0])\n",
    "        return torch.tensor(0, dtype=torch.float, device=gwr.device)\n",
    "\n",
    "    dis_weight = (\n",
    "        torch.sum(\n",
    "            1 - torch.sum(gwr * gws, dim=-1) / (torch.norm(gwr, dim=-1) * torch.norm(gws, dim=-1) + 0.000001)\n",
    "        )\n",
    "    )\n",
    "    dis = dis_weight\n",
    "    return dis\n",
    "\n",
    "\n",
    "def match_loss(\n",
    "        gradient_weights_syn: Tuple[torch.Tensor, ...], \n",
    "        gradient_weights_real: Tuple[torch.Tensor, ...], \n",
    "        device\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "    distance = torch.tensor(0.0).to(device)\n",
    "\n",
    "    for ig in range(len(gradient_weights_real)):\n",
    "        gwr = gradient_weights_real[ig]\n",
    "        gws = gradient_weights_syn[ig]\n",
    "        distance += distance_wb(gwr, gws)\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "K = 1 # number of initialization\n",
    "T = 10 # number of epochs\n",
    "C = 100 # number of minibatches\n",
    "\n",
    "def initialize_model() -> GPT2LMHeadModel:\n",
    "    pass\n",
    "\n",
    "def sample_batch(real_data, syn_data):\n",
    "    # Samples a batch from Real and synthetic dataset. Both have the same class\n",
    "    pass\n",
    "\n",
    "def evaluate():\n",
    "    pass\n",
    "\n",
    "\n",
    "real_data = torch.Tensor()\n",
    "syn_data = torch.randint(0, 50257-1, (120, 128)).to(DEVICE)\n",
    "\n",
    "criterion_real = torch.nn.CrossEntropyLoss()\n",
    "criterion_syn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_create_syn = torch.nn.CrossEntropyLoss()\n",
    "optimizer_create_syn = torch.optim.SGD([syn_data, ], lr=0.1, momentum=0.5)\n",
    "\n",
    "\n",
    "for k in range(K):\n",
    "    model = initialize_model()\n",
    "    model.train()\n",
    "    optimizer_model = torch.optim.SGD(model.parameters())\n",
    "    optimizer_model.zero_grad()\n",
    "    model_parameters = list(model.parameters())\n",
    "    \n",
    "    for t in range(T): # Epochs\n",
    "\n",
    "        for batch_real, batch_syn in sample_batch(real_data, syn_data):\n",
    "            loss = torch.tensor(0.0).to(DEVICE)\n",
    "\n",
    "            # Compute the real loss and get real gradient weights\n",
    "            x_real = batch_real['input_ids']\n",
    "            attn_mask_real = batch_real['attention_mask']\n",
    "            y_real = x_real.clone()\n",
    "            out_real = model(x_real, attention_mask=attn_mask_real, labels=y_real)\n",
    "            loss_real = criterion_real(out_real, y_real)\n",
    "            gradient_weights_real = torch.autograd.grad(loss_real, model_parameters) # These are like the \"target values\" for the match_loss\n",
    "\n",
    "            # Compute the synthetic loss and get synthetic gradient weights\n",
    "            x_syn = batch_syn['input_ids']\n",
    "            y_syn = x_syn.clone()\n",
    "            syn_embed = model.get_input_embeddings()(x_syn) # Grab the synthetic embeddings, this is the vector space that we wish to update\n",
    "            syn_embed.requires_grad = True # It requires grads now\n",
    "            optimizer_data = torch.optim.SGD([syn_embed,]) # It is being tracked in an optimizer\n",
    "            out_syn = model(inputs_embeds=syn_embed, labels=y_syn) # We pass it through the remaining model\n",
    "            loss_syn = criterion_syn(out_syn, y_real)\n",
    "            gradient_weights_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True) # We obtain embeddings for all layers, including embeddings layer.\n",
    "\n",
    "            loss += match_loss(\n",
    "                gradient_weights_syn, \n",
    "                gradient_weights_real, \n",
    "                device=DEVICE\n",
    "            )\n",
    "\n",
    "            optimizer_data.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_data.step() # Update the synthetic embeddings\n",
    "\n",
    "            # Obtain Synthetic Tokens\n",
    "            syn_embed_train = copy.deepcopy(syn_embed.detach())\n",
    "            syn_data_train = decode_embeddings(model.get_input_embeddings(), syn_embed_train)\n",
    "            \n",
    "            # Now we can update our network using synthetic tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crux Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.tensor(0.0).to(DEVICE)\n",
    "\n",
    "# Compute the real loss and get real gradient weights\n",
    "x_real = batch_real['input_ids']\n",
    "attn_mask_real = batch_real['attention_mask']\n",
    "y_real = x_real.clone()\n",
    "out_real = model(x_real, attention_mask=attn_mask_real, labels=y_real)\n",
    "loss_real = criterion_real(out_real, y_real)\n",
    "gradient_weights_real = torch.autograd.grad(loss_real, model_parameters) # These are like the \"target values\" for the match_loss\n",
    "\n",
    "# Compute the synthetic loss and get synthetic gradient weights\n",
    "x_syn = batch_syn['input_ids']\n",
    "y_syn = x_syn.clone()\n",
    "syn_embed = model.get_input_embeddings()(x_syn) # Grab the synthetic embeddings, this is the vector space that we wish to update\n",
    "syn_embed.requires_grad = True # It requires grads now\n",
    "optimizer_data = torch.optim.SGD([syn_embed,]) # It is being tracked in an optimizer\n",
    "out_syn = model(inputs_embeds=syn_embed, labels=y_syn) # We pass it through the remaining model\n",
    "loss_syn = criterion_syn(out_syn, y_real)\n",
    "gradient_weights_syn = torch.autograd.grad(loss_syn, model_parameters, create_graph=True) # We obtain embeddings for all layers, including embeddings layer.\n",
    "\n",
    "loss += match_loss(\n",
    "    gradient_weights_syn, \n",
    "    gradient_weights_real, \n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "optimizer_data.zero_grad()\n",
    "loss.backward()\n",
    "optimizer_data.step() # Update the synthetic embeddings\n",
    "\n",
    "# Obtain Synthetic Tokens\n",
    "syn_embed_train = copy.deepcopy(syn_embed.detach())\n",
    "syn_data_train = decode_embeddings(model.get_input_embeddings(), syn_embed_train)\n",
    "\n",
    "# Now we can update our network using synthetic tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
