{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "model_id = \"gpt2\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/Atul Gandre/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4460b0ca85548bca2e00fa4de347ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Note: this is the same dataset as https://urldefense.com/v3/__https://pytorch.org/text/stable/datasets.html*id22__;Iw!!LIr3w8kk_Xxm!oJNtg0Dcg0AZd7jpP-TKv-pOUtoxBQ668RwcOjO1YIHTzTC8ZBVbXkyntoc9YijqdBKbGukpcgzchLbesQ$\n",
    "\n",
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 122.4M parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=128,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\Atul Gandre\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-0ef9e62734397d3d.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Atul Gandre\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-516f1284a2d69512.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Atul Gandre\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-e234ab76a2a238b2.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Atul Gandre\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-dffbbba7d859db17.arrow\n",
      "Loading cached processed dataset at C:\\Users\\Atul Gandre\\.cache\\huggingface\\datasets\\wikitext\\wikitext-2-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-e3a7ad8cda04b41f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be2887b302b45319a35958a84be01ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2461 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_dataset = (raw_dataset\n",
    "    .filter(lambda example: len(example['text']) > 0)\n",
    "    .map(\n",
    "        lambda example: tokenizer(\n",
    "            example['text'], \n",
    "            max_length=128, \n",
    "            truncation=True, \n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    ")\n",
    "\n",
    "processed_dataset = processed_dataset.remove_columns('text')\n",
    "processed_dataset.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"wikitext-2-gpt2\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5_000,\n",
    "    logging_steps=5_000,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=5_000,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=processed_dataset[\"train\"],\n",
    "    eval_dataset=processed_dataset[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def perplexity(model, dataloader: DataLoader, device: str):\n",
    "  losses = []\n",
    "  for i, data in enumerate(dataloader):\n",
    "    if i == 10:\n",
    "      break\n",
    "    seq_len = data['input_ids'].size(1)\n",
    "\n",
    "    input_ids = data['input_ids'].to(device)\n",
    "    attention_mask = data['attention_mask'].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:-seq_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids, attention_mask=attention_mask, labels=target_ids)\n",
    "      \n",
    "      shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "      shift_labels = target_ids[..., 1:].contiguous()\n",
    "      shift_attn_mask = attention_mask[..., 1:].contiguous()\n",
    "      \n",
    "      # Flatten the tokens\n",
    "      loss = torch.nn.functional.cross_entropy(\n",
    "          shift_logits.permute(0, 2, 1), \n",
    "          shift_labels, \n",
    "          reduction='none'\n",
    "      ) * shift_attn_mask\n",
    "\n",
    "    losses.append(loss.sum(dim=-1)/shift_attn_mask.sum(-1))\n",
    "\n",
    "  return torch.exp(torch.stack(losses).mean())\n",
    "\n",
    "dataloader = DataLoader(processed_dataset['validation'], batch_size=4)\n",
    "perplexity(model, dataloader, \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
