{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120, 128, 768]), torch.Size([52000, 768]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Dummy Embeddings\n",
    "embeddings = torch.nn.Embedding(52000, 768).to(device=DEVICE)\n",
    "\n",
    "# Dummy Synthetic Tokens\n",
    "synthetic_tokens = torch.randint(0, 52000-1, (120, 128)).to(DEVICE)\n",
    "\n",
    "# Compute Synthetic Data\n",
    "### Synthetic Data is the actual syn_data that is updated.\n",
    "synthetic_data = embeddings(synthetic_tokens)\n",
    "synthetic_data.shape, embeddings.weight.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Decoding Scheme for Embedding to Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_embeddings(embeddings: torch.nn.Embedding, embedded_data: torch.Tensor):\n",
    "    num_sentences = embedded_data.shape[0]\n",
    "    sentence_len = embedded_data.shape[1]\n",
    "\n",
    "    sentences = torch.zeros(num_sentences, sentence_len, device=DEVICE, dtype=torch.long)\n",
    "\n",
    "    for i in range(num_sentences):\n",
    "        sentence = torch.cdist(embedded_data[i, :, :].to(DEVICE), embeddings.weight.to(DEVICE), p=2)\n",
    "        sentences[i] = sentence.argmin(-1)\n",
    "    \n",
    "    return sentences.type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do some operations on synthetic data\n",
    "\n",
    "# Update synthetic data\n",
    "\n",
    "# Convert synthetic data into new synthetic tokens.\n",
    "new_synthetic_tokens = decode_embeddings(embeddings, synthetic_data)\n",
    "\n",
    "# Verify for non updated\n",
    "(new_synthetic_tokens == synthetic_tokens).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32203,  3636,  5115,  ..., 18433, 18306, 10781],\n",
       "        [28191, 34624, 15761,  ..., 25586,  3374, 44167],\n",
       "        [49045, 48424, 42490,  ..., 24051,  4248, 10695],\n",
       "        ...,\n",
       "        [ 5118,  1574, 18591,  ..., 37280, 39965, 34653],\n",
       "        [24316, 30113,  8084,  ..., 10403, 17517, 27080],\n",
       "        [ 1594, 33412, 32000,  ...,  5732, 25073, 49251]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_synthetic_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy Synthetic Tokens\n",
    "synthetic_tokens = torch.rand((120, 128, 50257), device=DEVICE, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 128, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Dummy Synthetic Tokens\n",
    "# synthetic_tokens = torch.rand((120, 128, 50257)).to(DEVICE)\n",
    "synthetic_tokens.requires_grad = True\n",
    "# Dummy Embeddings\n",
    "embeddings = torch.nn.Embedding(50257, 768).to(device=DEVICE)\n",
    "\n",
    "# Compute Embeddings\n",
    "em = synthetic_tokens @ embeddings.weight\n",
    "em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7802, 0.4084, 0.3867,  ..., 0.8092, 0.2666, 0.3060], device='cuda:0',\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.88 GiB (GPU 0; 4.00 GiB total capacity; 3.28 GiB already allocated; 0 bytes free; 3.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(synthetic_tokens, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32md:\\USC\\College\\Courses\\CSCI-544 Applied Natural Language Processing\\homework\\3\\src\\env\\lib\\site-packages\\torch\\nn\\functional.py:1841\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1839\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1840\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1841\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1842\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1843\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.88 GiB (GPU 0; 4.00 GiB total capacity; 3.28 GiB already allocated; 0 bytes free; 3.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16430, 26160, 33138,  ..., 45060, 43348, 23012],\n",
       "        [10364, 43655,  6679,  ..., 10967, 40995, 12474],\n",
       "        [ 9899,  9064, 12806,  ..., 45797, 30795, 42207],\n",
       "        ...,\n",
       "        [12166, 16661,  9282,  ..., 16306, 25395, 15862],\n",
       "        [24887, 23780, 23485,  ..., 10882, 44750,  5158],\n",
       "        [40683, 35154, 14796,  ...,  9812, 17618, 31652]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthetic_tokens.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
